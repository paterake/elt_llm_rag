# Shared RAG Configuration

# ChromaDB settings
chroma:
  persist_dir: "../chroma_db"
  tenant: "rag_tenants"
  database: "knowledge_base"

# Ollama settings
ollama:
  base_url: "http://localhost:11434"
  embedding_model: "nomic-embed-text"  # keep this, it's good
  llm_model: "qwen2.5:14b"      # ~9GB â€” strong document synthesis and instruction following
  embed_batch_size: 1
  context_window: 8192          # enough for 10-15 retrieved chunks
  request_timeout: 300.0        # 5 minutes for complex multi-collection queries

# Chunking settings
chunking:
  strategy: "sentence"  # or "semantic"
  chunk_size: 256
  chunk_overlap: 32
  sentence_split_threshold: 0.5

# Query settings
query:
  similarity_top_k: 10
  use_hybrid_search: true  # Combine BM25 keyword + vector search (fixes structured/list content)
  use_reranker: true        # Re-score retrieved chunks before LLM synthesis
  reranker_strategy: "embedding"  # "embedding" (Ollama/local) | "cross-encoder" (needs HuggingFace)
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"  # used only when strategy=cross-encoder
  reranker_retrieve_k: 20  # Candidates fetched before reranking
  reranker_top_k: 8        # Chunks kept after reranking and passed to LLM
  system_prompt: |
    You are a helpful assistant that answers questions based on the provided documents.
    Always ground your answers in the retrieved content.
    When the documents clearly list items (such as chapters, steps, or roles), enumerate them explicitly.
    When the documents only partially describe something, say that the list may be incomplete and explain what is stated.
    If the information is not available in the documents, say so clearly.
